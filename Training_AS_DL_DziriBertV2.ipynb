{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "979e6de5",
      "metadata": {
        "id": "979e6de5"
      },
      "source": [
        "<h1 style=\"text-align: center; color: #E30613;\"><b><i>Entra√Ænement avec DziriBert sur les Donn√©es de Sentiments</i></b></h1>\n",
        "\n",
        "<p style=\"font-size: 18px;\">\n",
        "Ce notebook pr√©sente un workflow complet pour l'entra√Ænement et l'√©valuation d'un mod√®le de classification des sentiments en utilisant <span style=\"color: #28A745;\"><b>DziriBert</b></span>, un mod√®le de langage pr√©-entra√Æn√©.\n",
        "</p>\n",
        "\n",
        "### <span style=\"color: #28A745;\">**Objectifs :**</span>\n",
        "1. Charger et pr√©traiter les donn√©es textuelles.\n",
        "2. Encoder les √©tiquettes des sentiments.\n",
        "3. Utiliser un tokenizer pour pr√©parer les donn√©es pour le mod√®le.\n",
        "4. Diviser les donn√©es en ensembles d'entra√Ænement, de validation et de test.\n",
        "5. Entra√Æner le mod√®le avec des techniques avanc√©es comme le dropout et l'early stopping.\n",
        "6. √âvaluer les performances du mod√®le √† l'aide de m√©triques comme la matrice de confusion et le rapport de classification.\n",
        "7. Visualiser les courbes de pertes et d'accuracy pour analyser les performances.\n",
        "\n",
        "### <span style=\"color: #28A745;\">**Plan du Notebook :**</span>\n",
        "1. **Introduction et Biblioth√®ques n√©cessaires**  \n",
        "    Importation des biblioth√®ques et configuration de l'environnement.\n",
        "    \n",
        "2. **Chargement et Pr√©traitement des Donn√©es**  \n",
        "    Nettoyage des donn√©es et pr√©paration des √©tiquettes.\n",
        "\n",
        "3. **Tokenization et Pr√©paration des Donn√©es**  \n",
        "    Utilisation du tokenizer DziriBert et cr√©ation des ensembles d'entra√Ænement, de validation et de test.\n",
        "\n",
        "4. **Entra√Ænement du Mod√®le**  \n",
        "    Entra√Ænement avec des techniques comme l'early stopping et le dropout.\n",
        "\n",
        "5. **√âvaluation et Visualisation**  \n",
        "    Analyse des performances avec des m√©triques et des visualisations.\n",
        "\n",
        "6. **Conclusion**  \n",
        "    R√©sum√© des r√©sultats et perspectives d'am√©lioration."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zYubPwo2ezaN",
      "metadata": {
        "id": "zYubPwo2ezaN"
      },
      "source": [
        "# <span style=\"color: #E30613;\">**DziriBert**</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68103f0f",
      "metadata": {
        "id": "68103f0f"
      },
      "source": [
        "## <span style=\"color: #28A745;\">**Bibilioth√®ques n√©cessaires**</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "915dbad9",
      "metadata": {
        "collapsed": true,
        "id": "915dbad9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torch.nn import functional as F\n",
        "from torch import nn\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig, get_scheduler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7WOHv2OTVsJ4",
      "metadata": {
        "id": "7WOHv2OTVsJ4"
      },
      "source": [
        "## <span style=\"color: #28A745;\">**Utilisation de GPU**</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "QckqKaljV0xd",
      "metadata": {
        "id": "QckqKaljV0xd"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Configuration du device GPU uniquement\n",
        "assert torch.cuda.is_available(), \"CUDA GPU is not available.\"\n",
        "device = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hUFyzWDFV-ea",
      "metadata": {
        "id": "hUFyzWDFV-ea"
      },
      "source": [
        "## <span style=\"color: #28A745;\">**Chargement des Donn√©es**</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "APU8OV_aWDGo",
      "metadata": {
        "id": "APU8OV_aWDGo"
      },
      "outputs": [],
      "source": [
        "# üìÅ Chargement des donn√©es\n",
        "df = pd.read_csv(\"/content/Results/Comments_clean.csv\").dropna(subset=[\"Comments\", \"Sentiments\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dpgLkOzNWIfI",
      "metadata": {
        "id": "dpgLkOzNWIfI"
      },
      "source": [
        "## <span style=\"color: #28A745;\">**Encodage des √©tiquettes**</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4skFs0KGWSUa",
      "metadata": {
        "id": "4skFs0KGWSUa"
      },
      "outputs": [],
      "source": [
        "# üéØ Encodage des √©tiquettes\n",
        "label_encoder = LabelEncoder()\n",
        "df[\"Sentiments_encoded\"] = label_encoder.fit_transform(df[\"Sentiments\"])\n",
        "num_labels = len(label_encoder.classes_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "v-EKAf3OWTy0",
      "metadata": {
        "id": "v-EKAf3OWTy0"
      },
      "source": [
        "## <span style=\"color: #28A745;\">**Tokenizer, Split dataset (Train 70%, Test 20%, Val 10%)**</span>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "bJ667pVwOWYp",
        "outputId": "650157e0-9423-4e75-b1c8-5de7e2443490",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "bJ667pVwOWYp",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "12b41a4c",
      "metadata": {
        "id": "12b41a4c",
        "outputId": "7f757c64-567b-4195-ee75-6f237d6754c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import wordnet\n",
        "import random\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# Charger le tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"alger-ia/dziribert\")\n",
        "\n",
        "\n",
        "import random\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "def get_synonyms(word):\n",
        "    \"\"\" Trouve les synonymes d'un mot en utilisant WordNet. \"\"\"\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.add(lemma.name())\n",
        "    if word in synonyms:\n",
        "        synonyms.remove(word)\n",
        "    return list(synonyms)\n",
        "\n",
        "def augment_text(text, p_swap=0.2, p_del=0.1, p_syn=0.15):\n",
        "    \"\"\" Applique diverses augmentations sur le texte. \"\"\"\n",
        "    words = text.split()\n",
        "    if len(words) < 2: return text\n",
        "\n",
        "    # üîÑ 1Ô∏è‚É£ Remplacement par synonymes\n",
        "    if random.random() < p_syn:\n",
        "        for i, word in enumerate(words):\n",
        "            synonyms = get_synonyms(word)\n",
        "            if synonyms and random.random() > 0.5:\n",
        "                words[i] = random.choice(synonyms)\n",
        "\n",
        "    # üîÄ 2Ô∏è‚É£ Permutation de mots adjacents\n",
        "    if random.random() < p_swap:\n",
        "        idx = list(range(len(words) - 1))\n",
        "        random.shuffle(idx)\n",
        "        for i in idx[:max(1, len(idx) // 5)]:\n",
        "            words[i], words[i + 1] = words[i + 1], words[i]\n",
        "\n",
        "    # ‚ùå 3Ô∏è‚É£ Suppression al√©atoire de mots (maximum 2 mots)\n",
        "    if random.random() < p_del and len(words) > 2:\n",
        "        idx_to_remove = random.sample(range(len(words)), k=min(2, len(words) // 4))\n",
        "        words = [word for i, word in enumerate(words) if i not in idx_to_remove]\n",
        "\n",
        "    return \" \".join(words)\n",
        "\n",
        "\n",
        "# üéØ **Dataset avec augmentation**\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128, augment=False):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "\n",
        "        # ‚û°Ô∏è Appliquer l'augmentation seulement pendant l'entra√Ænement\n",
        "        if self.augment:\n",
        "            text = augment_text(text)\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# ‚úÇÔ∏è Split dataset (Train 70%, Test 20%, Val 10%)\n",
        "dataset = SentimentDataset(df[\"Comments\"].values, df[\"Sentiments_encoded\"].values, tokenizer, augment=True)\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.1 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# ‚û°Ô∏è Charger les donn√©es\n",
        "train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=16)\n",
        "test_loader = DataLoader(test_set, batch_size=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "557123f1",
      "metadata": {
        "id": "557123f1"
      },
      "source": [
        "## <span style=\"color: #28A745;\">**Chargement du mod√®le avec dropout custom**</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "eecd3412",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eecd3412",
        "outputId": "fde48d60-5b2b-4a77-c5f3-74a76ac9caf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at alger-ia/dziribert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(50000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.4, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.4, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.4, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.4, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "\"\"\"\n",
        "# Chargement du mod√®le avec dropout custom\n",
        "config = AutoConfig.from_pretrained(\"alger-ia/dziribert\", num_labels=num_labels, hidden_dropout_prob=0.45)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"alger-ia/dziribert\", config=config)\n",
        "model.to(device)\n",
        "\"\"\"\n",
        "\n",
        "config = AutoConfig.from_pretrained(\"alger-ia/dziribert\", num_labels=num_labels, hidden_dropout_prob=0.4, output_attentions=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"alger-ia/dziribert\", config=config)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b412658",
      "metadata": {
        "id": "8b412658"
      },
      "source": [
        "## <span style=\"color: #28A745;\">**Optimiseur et Scheduler**</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "267e3ffc",
      "metadata": {
        "id": "267e3ffc"
      },
      "outputs": [],
      "source": [
        "# Optimiseur et Scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "num_epochs = 10\n",
        "num_training_steps = num_epochs * len(train_loader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"cosine\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=100,\n",
        "    num_training_steps=num_training_steps\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d4197a4",
      "metadata": {
        "id": "3d4197a4"
      },
      "source": [
        "## <span style=\"color: #28A745;\">**Entra√Ænement de Mod√®le**</span>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def mixup_data(x, y, alpha=0.1):\n",
        "    \"\"\" M√©lange les donn√©es et les labels avec un coefficient al√©atoire. \"\"\"\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam"
      ],
      "metadata": {
        "id": "lqm3lS0qY9Ow"
      },
      "id": "lqm3lS0qY9Ow",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d571625b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d571625b",
        "outputId": "ad401d5b-2c16-4b5f-e7d8-f9e5b4781cd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üü¢ Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   0%|          | 0/159 [00:00<?, ?it/s]BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "Training:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 75/159 [00:30<00:31,  2.71it/s]"
          ]
        }
      ],
      "source": [
        "# üîß Entra√Ænement avec Early Stopping\n",
        "best_val_loss = float(\"inf\")\n",
        "patience, patience_counter = 2, 0\n",
        "train_losses, val_losses = [], []\n",
        "train_accs, val_accs = [], []\n",
        "\n",
        "# Entra√Ænement avec Mixup et Dropout\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nüü¢ Epoch {epoch+1}/{num_epochs}\")\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # üí• Mixup - Appliquer uniquement pendant l'entra√Ænement\n",
        "        if random.random() < 0.5:\n",
        "            mixed_input_ids, labels_a, labels_b, lam = mixup_data(input_ids.float(), labels)\n",
        "            mixed_input_ids = mixed_input_ids.long()  # Corriger le type apr√®s mixup\n",
        "            outputs = model(input_ids=mixed_input_ids, attention_mask=attention_mask)\n",
        "            loss = lam * criterion(outputs.logits, labels_a) + (1 - lam) * criterion(outputs.logits, labels_b)\n",
        "        else:\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "        logits = outputs.logits\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    train_loss = total_loss / len(train_loader)\n",
        "    train_acc = correct / total\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "\n",
        "    # üîç Validation (sans Mixup)\n",
        "    model.eval()\n",
        "    val_loss, correct, total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    val_loss = val_loss / len(val_loader)\n",
        "    val_acc = correct / total\n",
        "    val_losses.append(val_loss)\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "    print(f\"‚úÖ Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f} - Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # üõë Early Stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), \"/content/Results/best_model_DziriBert.pt\")\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"‚õî Early stopping triggered.\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2aa2587",
      "metadata": {
        "id": "b2aa2587"
      },
      "source": [
        "## <span style=\"color: #28A745;\">**√âvaluation finale**</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4708042f",
      "metadata": {
        "id": "4708042f"
      },
      "outputs": [],
      "source": [
        "# √âvaluation finale\n",
        "model.load_state_dict(torch.load(\"/content/Results/best_model_DziriBert.pt\"))\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "for batch in test_loader:\n",
        "    input_ids = batch[\"input_ids\"].to(device)\n",
        "    attention_mask = batch[\"attention_mask\"].to(device)\n",
        "    labels = batch[\"labels\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    preds = torch.argmax(outputs.logits, dim=1)\n",
        "    all_preds.extend(preds.cpu().numpy())\n",
        "    all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "print(\"üîç Rapport de classification pour DziriBert:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3be60881",
      "metadata": {
        "id": "3be60881"
      },
      "source": [
        "## <span style=\"color: #28A745;\">**Courbes des pertes**</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51acc255",
      "metadata": {
        "id": "51acc255"
      },
      "outputs": [],
      "source": [
        "# Courbes des pertes\n",
        "plt.plot(train_losses, label=\"Train Loss\", color='#28A745')  # Vert\n",
        "plt.plot(val_losses, label=\"Validation Loss\", color='#E30613')  # Rouge\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f42c89f",
      "metadata": {
        "id": "0f42c89f"
      },
      "source": [
        "## <span style=\"color: #28A745;\">**Matrice de confusion**</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "067ff487",
      "metadata": {
        "id": "067ff487"
      },
      "outputs": [],
      "source": [
        "# Matrice de confusion\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_, cmap=\"Reds\")\n",
        "plt.xlabel(\"Pr√©dictions\")\n",
        "plt.ylabel(\"V√©rit√©s\")\n",
        "plt.title(\"Matrice de confusion - DziriBert\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fcc10c4",
      "metadata": {
        "id": "0fcc10c4"
      },
      "source": [
        "<h3 style=\"text-align: center; color: #E30613;\"><b><i>D√©velopp√© par: OUARAS Khelil Rafik</i></b></h3>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}